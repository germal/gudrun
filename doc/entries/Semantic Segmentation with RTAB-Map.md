# Semantic Segmentation with RTAB-Map

I'm interested in running a semantic segmentation ANN alongside RTAB-Map, and would like to get some advice on how I should plan to integrate this into the rest of RTAB-Map. Ideally, I'd like for this to supplant the existing surface-normal based ground/obstacle segmentation. 

I'm currently using a first-generation Kinect RGB-D sensor. Assume, for the sake of this discussion, that I have a black box that can label the pixels in the RGB feed from this camera as either drivable space, (semi-)permanent obstacle (e.g. buildings or furniture), possibly also transient obstacles (people, vehicles, or animals), and unknown/uncertain. This could be encoded for instance as an alternative color channel (though not one that will be useful for the AB part of RTAB), with big monochromatic splotches of category colors. Likely these categorizations arrive more slowly than the RGB(D) data; maybe 10Hz.

I've been reading over the code, and I would like some advice on where I should try to add this data. I see in [MapsManager.cpp](https://github.com/introlab/rtabmap_ros/blob/0.17.6-melodic/src/MapsManager.cpp#L1083) where the full map is published as conversions of `pcl::PointXYZRGB`, although this is downstream of the segmenter (either the simple height filter or the surface-normal filter in [OccupancyGrid.hpp](https://github.com/introlab/rtabmap/blob/0.17.6-melodic/corelib/include/rtabmap/core/impl/OccupancyGrid.hpp#L115)).

I'm not opposed to either doing this all in Python as a separate ROS node, redundant with the normals segmentation, or in C++ as a personal fork of the RTAB-Map code. I gather that the second might be preferable primarily because the existing segmentation (resulting projected occupancy grid for `move_base` to use) is regenerated regularly, since the underlying data structure is not a voxelized 3D map, but a pose graph with associated node data. Would it be possible to extend the list of fields in that point cloud to include, say, a one-byte categorization field?

In [OccupancyGrid.cpp](https://github.com/introlab/rtabmap/blob/0.17.6-melodic/corelib/src/OccupancyGrid.cpp#L443), the function `segmentCloud` is called with four different template parameters depending on the fields available, although the [actual implementation](https://github.com/introlab/rtabmap/blob/0.17.6-melodic/corelib/include/rtabmap/core/impl/OccupancyGrid.hpp#L38) doesn't seem to have any sort of multiple dispatch based on this, and these are not ROS PointCloud2 clouds, which allow for an arbitrary number of fields, a number of different `pcl::Point***` types with their fields given explicitly in their type names. Also, though I'm not exactly sure how we get to this point in the code (perhaps when either the [`getMapCallback`](https://github.com/introlab/rtabmap_ros/blob/master/src/CoreWrapper.cpp#L2655) or `MapsManager::updateMapCaches` is called, `segmentCloud` is called repeatedly for little local views, and the whole map is stitched together). However, one possible call path I trace, that conflicts with this theory, is `Rtabmap::process -> `(or maybe `OdometryMono::computeTransform ->`) `Memory::update -> Memory::createSignature -> OccupancyGrid::createLocalMap` (receives full node as `Signature` and can get `SensorData` from it) -> `OccupancyGrid::segmentCloud`.

Could this category information be stored in the database as UserData? I see in SensorData.cpp that, while there are numerous way to initialize (e.g., the only difference between the RGB-D and Stereo constructors whether the third argument is type `CameraModel` or `StereoCameraModel`; this polymorphism might eventually be a better way to implement this feature), the `UserData` argument is always, like the `rgb`, `depth`, `left`, and `right` arguments, a `const cv::Mat &`, and so can hold a whole image instead of the scalar wifi-strength example given in the tutorials. Does this UserData get provided to the segmenter? And, how can I be sure that my ANN-segmented user data is added to the same node that contains the RGB data used to create it?

Perhaps, given the call path I listed above, a workable design would be to modify `createLocalMap` to create the classification image at that time, and then use an alternative  `OccupancyGrid::segmentCloud`. However, since this takes a tenth of a second, I'd be worried about holding up other parts of the process. How frequently is this path called, and is there perhaps the possibility that the path will be called repeatedly on nodes that have already been seen, in which case we can skip the ANN?

Thanks 